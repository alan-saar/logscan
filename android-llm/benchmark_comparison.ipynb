{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação de Benchmark Logscan\n",
    "\n",
    "Este notebook compara a precisão do Logscan em dois datasets:\n",
    "1. **Loghub 2k** (Dataset original, resultados em `Logscan_benchmark_result.csv`)\n",
    "2. **Loghub 2.0** (Dataset atualizado, resultados em `Logscan-llm_benchmark_result.csv`)\n",
    "\n",
    "O objetivo é analisar o impacto da atualização do dataset na precisão do parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Caminhos para os arquivos de resultados\n",
    "result_2k_path = \"../Logscan_benchmark_result.csv\"\n",
    "result_2_0_path = \"../Logscan-llm_benchmark_result.csv\"\n",
    "\n",
    "# Verificar se os arquivos existem\n",
    "has_2k = os.path.exists(result_2k_path)\n",
    "has_2_0 = os.path.exists(result_2_0_path)\n",
    "\n",
    "if not has_2k and not has_2_0:\n",
    "    print(\"Nenhum arquivo de benchmark encontrado. Execute 'python -m logscan' e 'python -m logscan-llm' na raiz para gerá-los.\")\n",
    "else:\n",
    "    dfs = []\n",
    "    if has_2k:\n",
    "        df_2k = pd.read_csv(result_2k_path)\n",
    "        df_2k = df_2k.rename(columns={'Accuracy': 'Accuracy_2k'})\n",
    "        if 'Dataset' not in df_2k.columns and df_2k.index.name == 'Dataset':\n",
    "             df_2k = df_2k.reset_index()\n",
    "        dfs.append(df_2k.set_index('Dataset'))\n",
    "    \n",
    "    if has_2_0:\n",
    "        df_2_0 = pd.read_csv(result_2_0_path)\n",
    "        df_2_0 = df_2_0.rename(columns={'Accuracy': 'Accuracy_2.0'})\n",
    "        if 'Dataset' not in df_2_0.columns and df_2_0.index.name == 'Dataset':\n",
    "             df_2_0 = df_2_0.reset_index()\n",
    "        dfs.append(df_2_0.set_index('Dataset'))\n",
    "    \n",
    "    # Combinar os dataframes\n",
    "    if len(dfs) == 2:\n",
    "        df_compare = pd.concat(dfs, axis=1, join='inner') # 'inner' para comparar apenas datasets comuns\n",
    "        df_compare['Diferença'] = df_compare['Accuracy_2.0'] - df_compare['Accuracy_2k']\n",
    "        \n",
    "        print(\"Tabela de Comparação:\")\n",
    "        display(df_compare)\n",
    "        \n",
    "        # Plotar\n",
    "        ax = df_compare[['Accuracy_2k', 'Accuracy_2.0']].plot(kind='bar', figsize=(12, 6))\n",
    "        plt.title('Comparação de Acurácia Logscan: Loghub 2k vs Loghub 2.0')\n",
    "        plt.ylabel('Acurácia')\n",
    "        plt.xlabel('Dataset')\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nAnálise:\")\n",
    "        for dataset, row in df_compare.iterrows():\n",
    "            diff = row['Diferença']\n",
    "            status = \"melhorou\" if diff > 0 else \"piorou\" if diff < 0 else \"manteve\"\n",
    "            print(f\"- {dataset}: {status} ({diff:+.4f})\")\n",
    "            \n",
    "    elif has_2k:\n",
    "        print(\"Apenas resultados do Loghub 2k encontrados:\")\n",
    "        display(df_2k)\n",
    "    elif has_2_0:\n",
    "        print(\"Apenas resultados do Loghub 2.0 encontrados:\")\n",
    "        display(df_2_0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
